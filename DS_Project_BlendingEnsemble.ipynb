{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qm0TtOzkZBy9",
    "outputId": "4af6693a-1ddb-4fe5-9604-c2b4e772cdd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YnNK0Ay4Z0AH"
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = '/content/gdrive/My Drive/DS_Project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Vs86O6sbZ2X4"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yja-JOn74jax"
   },
   "outputs": [],
   "source": [
    "seed = 142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "799iwF-04lsh",
    "outputId": "3810aee5-2054-40a2-ff49-b1cc4397fb91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1+cu121\n"
     ]
    }
   ],
   "source": [
    "# importing required libraries\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math,copy,re\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import torch.nn.init as init\n",
    "# from torchviz import make_dot\n",
    "# import seaborn as sns\n",
    "# import torchtext\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.simplefilter(\"ignore\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "svNHrPeP4oEf",
    "outputId": "3b062100-0a9c-483e-f75a-d76151dd4b2f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.24.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "e9kW-6fr4qaO"
   },
   "outputs": [],
   "source": [
    "# ML Algorithms used\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tWquP5y14sbu",
    "outputId": "4ab8faf2-287b-441f-c057-93f3f4e9fb8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is using GPU.\n",
      "Tensor device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # CUDA is available, so PyTorch can use a GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"PyTorch is using GPU.\")\n",
    "else:\n",
    "    # CUDA is not available, PyTorch will use CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"PyTorch is using CPU.\")\n",
    "\n",
    "# Create a tensor and move it to the device\n",
    "tensor = torch.randn(3, 4).to(device)\n",
    "\n",
    "# Check the device of the tensor\n",
    "print(\"Tensor device:\", tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ImpRQCEU4uxw"
   },
   "outputs": [],
   "source": [
    "def reset_random_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "#     tf.random.set_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "O8ckQNyy4xMR"
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OBjyQsLq4zNE"
   },
   "outputs": [],
   "source": [
    "#Rearrange the Array\n",
    "def makeArray(Array):\n",
    "    New=np.array(Array[0])\n",
    "\n",
    "    for i in range(1,len(Array)):\n",
    "        New = np.append(New,Array[i],axis=0)\n",
    "\n",
    "    return New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jKWhkspE6JME"
   },
   "outputs": [],
   "source": [
    "def readData(Stations):\n",
    "\n",
    "    Data, C = [], []\n",
    "\n",
    "    print(Stations)\n",
    "    df = pd.read_excel(Stations, header=0, index_col=None)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df[['EntityID','OriginalCreditor','AccountID','CurrentBalance','DebtLoadPrincipal','Balanaceatdebt_load','PurchasePrice','ProductOrDebtType','CollectionStatus','ClosureReason','InBankruptcy','AccountInsolvencyType','CustomerInsolvencyType','IsLegal','LastPaymentAmount','LastPaymentMethod','NumLiableParties','CustomerAge','NumPhones','NumEmails','NumAddresses','IsStatBarred']]\n",
    "    df = df.drop('EntityID', axis=1)\n",
    "    df = df.drop('AccountID', axis=1)\n",
    "    df = df.drop('AccountInsolvencyType', axis=1)\n",
    "    df = df.drop('NumLiableParties', axis=1)\n",
    "    df = df.drop('CustomerAge', axis=1)\n",
    "    df = df.drop('CustomerInsolvencyType', axis=1)\n",
    "    df = df.drop('InBankruptcy', axis=1)\n",
    "    df = df.drop('IsLegal', axis=1)\n",
    "    df = df.drop('DebtLoadPrincipal', axis=1)\n",
    "    df = df.drop('NumAddresses', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # Iterate through each column\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':  # Check if column is categorical\n",
    "        # Create a dictionary to map categories to labels\n",
    "            category_labels = {category: i+1 for i, category in enumerate(df[column].dropna().unique())}\n",
    "\n",
    "        # Assign labels to the column\n",
    "            df[column] = df[column].map(category_labels)\n",
    "            df[column].fillna(0, inplace=True)\n",
    "\n",
    "        else:\n",
    "            df[column].fillna(df[column].mean(), inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    data=df.values\n",
    "\n",
    "\n",
    "    Data.append(data)\n",
    "\n",
    "\n",
    "\n",
    "    return makeArray(Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BUotkjBd6pwD"
   },
   "outputs": [],
   "source": [
    "Folder = '/content/gdrive/My Drive/DS_Project/'\n",
    "Stations=os.listdir(Folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KYd7C5p160Bc",
    "outputId": "d95d5a34-b82f-4cd0-c40b-6ced6334d0b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Company_x.xlsx']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FwGgzji363gn",
    "outputId": "b31e4d40-9626-49de-a864-65266b7c312f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/DS_Project/Company_x.xlsx\n"
     ]
    }
   ],
   "source": [
    "Data=[[] for x in range(len(Stations))]\n",
    "for i in range(len(Stations)):\n",
    "    Data[i] = readData(Folder+Stations[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kobQ0EMZ66Gt",
    "outputId": "9c8cf2a5-d508-40ff-f113-bea63a010356"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406423\n",
      "[[1.00000e+00 0.00000e+00 1.16020e+03 ... 0.00000e+00 0.00000e+00\n",
      "  1.00000e+00]\n",
      " [2.00000e+00 1.82900e+02 1.82900e+02 ... 0.00000e+00 0.00000e+00\n",
      "  2.00000e+00]\n",
      " [1.00000e+00 0.00000e+00 5.38570e+02 ... 1.00000e+00 0.00000e+00\n",
      "  1.00000e+00]\n",
      " ...\n",
      " [5.00000e+01 3.51260e+03 3.51260e+03 ... 1.00000e+00 1.00000e+00\n",
      "  1.00000e+00]\n",
      " [5.00000e+01 4.47731e+03 4.47731e+03 ... 1.00000e+00 1.00000e+00\n",
      "  1.00000e+00]\n",
      " [5.00000e+01 2.72590e+02 2.72590e+02 ... 0.00000e+00 1.00000e+00\n",
      "  1.00000e+00]]\n",
      "Hello\n",
      "[[0.00000000e+00 1.71722786e-02 1.37408612e-03 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.96078431e-02 1.75792668e-02 2.16618128e-04 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+00]\n",
      " [0.00000000e+00 1.71722786e-02 6.37856890e-04 ... 1.25000000e-01\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " ...\n",
      " [9.60784314e-01 2.49885002e-02 4.16015766e-03 ... 1.25000000e-01\n",
      "  2.00000000e-01 0.00000000e+00]\n",
      " [9.60784314e-01 2.71351685e-02 5.30271466e-03 ... 1.25000000e-01\n",
      "  2.00000000e-01 0.00000000e+00]\n",
      " [9.60784314e-01 1.77788446e-02 3.22842731e-04 ... 0.00000000e+00\n",
      "  2.00000000e-01 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "L_shape=[]\n",
    "for i in range(len(Data)):\n",
    "    L_shape.append(Data[i].shape[0])\n",
    "\n",
    "D = np.concatenate(Data,axis=0)\n",
    "print(len(D))\n",
    "print(D)\n",
    "D = scaler.fit_transform(D)\n",
    "print(\"Hello\")\n",
    "print(D)\n",
    "s=0\n",
    "Normalize = []\n",
    "for i in range(len(L_shape)):\n",
    "    Normalize.append(D[s:s+L_shape[i]])\n",
    "    s=L_shape[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u9NRG8Nd68nI",
    "outputId": "8e0cd3ee-ae6e-4d34-98d5-08c1bed9c2cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(406423, 12)\n"
     ]
    }
   ],
   "source": [
    "L1_shape=[]\n",
    "for i in range(len(Normalize)):\n",
    "    L1_shape.append(Data[i].shape[0])\n",
    "\n",
    "DN = np.concatenate(Normalize,axis=0)\n",
    "print(DN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_Gf_7O26-gh",
    "outputId": "cc551699-26b3-47df-80e4-fb27e8c74995"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of 0's 121875\n",
      "Total Number of 1's 284548\n",
      "121875\n",
      "284548\n"
     ]
    }
   ],
   "source": [
    "count0=0\n",
    "count1=0\n",
    "index0=[]\n",
    "index1=[]\n",
    "for i in range(len(DN)):\n",
    "    if DN[i][11]==0:\n",
    "        index0.append(i)\n",
    "        count0+=1\n",
    "    elif DN[i][11]==1:\n",
    "        index1.append(i)\n",
    "        count1+=1\n",
    "\n",
    "\n",
    "\n",
    "print(\"Total Number of 0's\", count0)\n",
    "print(\"Total Number of 1's\", count1)\n",
    "print(len(index0))\n",
    "print(len(index1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kk6kDqVT7APV",
    "outputId": "0dce2023-12a8-495f-8a72-3c21c2e55dd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(406423, 11)\n",
      "(406423,)\n"
     ]
    }
   ],
   "source": [
    "X_full=DN[:, :-1]\n",
    "y_full=DN[:,-1]\n",
    "print(X_full.shape)\n",
    "print(y_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "2cKVmMEf7CnO"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "DK6UnDHR7EVQ"
   },
   "outputs": [],
   "source": [
    "X_train=np.array(X_train)\n",
    "X_test=np.array(X_test)\n",
    "y_train=np.array(y_train)\n",
    "y_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxxIpsHw7F8R",
    "outputId": "9862532d-7328-4260-ce05-2ed0b56fc96a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "[0. 1.]\n",
      "(325138, 11)\n",
      "(325138,)\n",
      "(81285, 11)\n",
      "(81285,)\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_train))\n",
    "print(np.unique(y_test))\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "_7v6tSry7XB3"
   },
   "outputs": [],
   "source": [
    "def get_dte_ensemble():\n",
    " # define the base models\n",
    "    models = list()\n",
    " # normalization\n",
    "    norm = Pipeline([('s', MinMaxScaler()), ('m', DecisionTreeClassifier())])\n",
    "    models.append(('norm', norm))\n",
    " # standardization\n",
    "    st = Pipeline([('s', StandardScaler()), ('m', DecisionTreeClassifier())])\n",
    "    models.append(('std', st))\n",
    " # robust\n",
    "    robust = Pipeline([('s', RobustScaler()), ('m', DecisionTreeClassifier())])\n",
    "    models.append(('robust', robust))\n",
    " # power\n",
    "    power = Pipeline([('s', PowerTransformer()), ('m', DecisionTreeClassifier())])\n",
    "    models.append(('power', power))\n",
    " # quantile\n",
    "    quant = Pipeline([('s', QuantileTransformer(n_quantiles=100,\n",
    "    output_distribution='normal')), ('m', DecisionTreeClassifier())])\n",
    "    models.append(('quant', quant))\n",
    "\n",
    " # define the voting ensemble\n",
    "    ensemble = VotingClassifier(estimators=models, voting='soft')\n",
    " # return a list of tuples each with a name and model\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "lEJ_bY2m7XlX"
   },
   "outputs": [],
   "source": [
    "def get_cfse_ensemble(n_features):\n",
    " # define the base models\n",
    "    models = list()\n",
    " # anova member\n",
    "    fs = SelectKBest(score_func=f_classif, k=n_features)\n",
    "    anova = Pipeline([('fs', fs), ('m', DecisionTreeClassifier())])\n",
    "    models.append(('anova', anova))\n",
    " # mutual information member\n",
    "    fs = SelectKBest(score_func=mutual_info_classif, k=n_features)\n",
    "    mutinfo = Pipeline([('fs', fs), ('m', DecisionTreeClassifier())])\n",
    "    models.append(('mutinfo', mutinfo))\n",
    " # rfe member\n",
    "    fs = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=n_features)\n",
    "    rfe = Pipeline([('fs', fs), ('m', DecisionTreeClassifier())])\n",
    "    models.append(('rfe', rfe))\n",
    " # define the voting ensemble\n",
    "    ensemble = VotingClassifier(estimators=models, voting='soft')\n",
    "\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "JBXbHv5T7cBW"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import StackingClassifier, GradientBoostingClassifier, ExtraTreesClassifier, RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.metrics import f1_score, make_scorer, accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve, auc\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AqDtMapx7Y7_",
    "outputId": "13ae796f-6d58-4403-b6ac-f25ff13c4e9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (217842, 11), Val: (107296, 11), Test: (81285, 11)\n",
      "Confusion Matrix:\n",
      " [[23608   701]\n",
      " [  949 56027]]\n",
      "Accuracy 0.9797010518545857\n",
      "F1 Score 0.9797304244481378\n",
      "Recall 0.9797010518545857\n",
      "Precision 0.9797812550644458\n",
      "Area Under the Curve 0.9978853090693259\n"
     ]
    }
   ],
   "source": [
    "from numpy import hstack\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    \n",
    "    models['cfse']=get_cfse_ensemble(10)\n",
    "    models['xgb']=XGBClassifier(random_state=42,n_estimators=500,eta=1,max_depth=10,subsample=1.0,colsample_bytree=1.0)\n",
    "    models['rse']=BaggingClassifier(bootstrap=False, max_features=0.8, n_estimators=500,random_state=42)\n",
    "    models['pe']=BaggingClassifier(bootstrap=False, max_samples=0.8, n_estimators=500,random_state=42)\n",
    "    models['random_forest']=RandomForestClassifier(bootstrap=True, max_depth=10, n_estimators=100,max_features=0.8,max_samples=0.8,random_state=42)\n",
    "    models['extra_tress']=ExtraTreesClassifier(min_samples_split=2, n_estimators=500,max_features=1.0,random_state=42)\n",
    "    models['dte']=get_dte_ensemble()\n",
    "    models['adaboost']=AdaBoostClassifier(random_state=42,n_estimators=500,learning_rate=0.2)\n",
    "    models['gradient']=GradientBoostingClassifier(random_state=42,n_estimators=500,learning_rate=1.0,max_depth=10,subsample=1.0,max_features=1.0)\n",
    "    models['rpe']=BaggingClassifier(bootstrap=False, max_features=10,n_estimators=500,max_samples=0.8,random_state=42)\n",
    "    members = [(n,m) for n,m in models.items()]\n",
    "    return members\n",
    "\n",
    "def fit_ensemble(models, X_train, X_val, y_train, y_val):\n",
    "\n",
    "    meta_X = list()\n",
    "    for _, model in models:\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        yhat = model.predict_proba(X_val)\n",
    "\n",
    "        meta_X.append(yhat)\n",
    "\n",
    "    meta_X = hstack(meta_X)\n",
    "\n",
    "    blender = GradientBoostingClassifier(random_state=42,n_estimators=500)\n",
    "\n",
    "    blender.fit(meta_X, y_val)\n",
    "    return blender\n",
    "\n",
    "def predict_ensemble(models, blender, X_test):\n",
    " # make predictions with base models\n",
    "    meta_X = list()\n",
    "    for _, model in models:\n",
    " # predict with base model\n",
    "        yhat = model.predict_proba(X_test)\n",
    " # store prediction\n",
    "        meta_X.append(yhat)\n",
    " # create 2d array from predictions, each set is an input feature\n",
    "    meta_X = hstack(meta_X)\n",
    " # predict\n",
    "    return blender.predict(meta_X)\n",
    "\n",
    "\n",
    "\n",
    "def predict_ensemble2(models, blender, X_test):\n",
    " # make predictions with base models\n",
    "    meta_X = list()\n",
    "    for _, model in models:\n",
    " # predict with base model\n",
    "        yhat = model.predict_proba(X_test)\n",
    " # store prediction\n",
    "        meta_X.append(yhat)\n",
    " # create 2d array from predictions, each set is an input feature\n",
    "    meta_X = hstack(meta_X)\n",
    " # predict\n",
    "    return meta_X\n",
    "\n",
    "X_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "print('Train: %s, Val: %s, Test: %s' % (X_train1.shape, X_val.shape, X_test.shape))\n",
    " # create the base models\n",
    "models = get_models()\n",
    " # train the blending ensemble\n",
    "blender = fit_ensemble(models, X_train1, X_val, y_train1, y_val)\n",
    " # make predictions on test set\n",
    "predictions = predict_ensemble(models, blender, X_test)\n",
    " # evaluate predictions\n",
    "f1 = f1_score(y_test, predictions, average='weighted')\n",
    "accuracy=accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions,average=\"weighted\")\n",
    "recall = recall_score(y_test, predictions,average=\"weighted\")\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "predictions_prob = blender.predict_proba(predict_ensemble2(models,blender,X_test))[:,1]\n",
    "\n",
    "fpr2, tpr2, _ = roc_curve(y_test,\n",
    "                          predictions_prob,\n",
    "                          pos_label = 1)\n",
    "auc1 = auc(fpr2, tpr2)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Accuracy\", accuracy)\n",
    "print(\"F1 Score\", f1)\n",
    "print(\"Recall\", recall)\n",
    "print(\"Precision\",precision)\n",
    "print(\"Area Under the Curve\",auc1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aEQ2jSxc7i_d",
    "outputId": "8a6ef0d9-d301-411a-8862-ee23c3bebf82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (217842, 11), Val: (107296, 11), Test: (81285, 11)\n",
      "Confusion Matrix:\n",
      " [[ 95176   2390]\n",
      " [  2996 224576]]\n",
      "Accuracy 0.9834347261778076\n",
      "F1 Score 0.9803171158707593\n",
      "Recall 0.9811693486564378\n",
      "Precision 0.9794759609124928\n",
      "Area Under the Curve 0.9987025041639424\n"
     ]
    }
   ],
   "source": [
    "from numpy import hstack\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    \n",
    "    models['cfse']=get_cfse_ensemble(10)\n",
    "    models['xgb']=XGBClassifier(random_state=42,n_estimators=500,eta=1,max_depth=10,subsample=1.0,colsample_bytree=1.0)\n",
    "    models['rse']=BaggingClassifier(bootstrap=False, max_features=0.8, n_estimators=500,random_state=42)\n",
    "    models['pe']=BaggingClassifier(bootstrap=False, max_samples=0.8, n_estimators=500,random_state=42)\n",
    "    models['random_forest']=RandomForestClassifier(bootstrap=True, max_depth=10, n_estimators=100,max_features=0.8,max_samples=0.8,random_state=42)\n",
    "    models['extra_tress']=ExtraTreesClassifier(min_samples_split=2, n_estimators=500,max_features=1.0,random_state=42)\n",
    "    models['dte']=get_dte_ensemble()\n",
    "    models['adaboost']=AdaBoostClassifier(random_state=42,n_estimators=500,learning_rate=0.2)\n",
    "    models['gradient']=GradientBoostingClassifier(random_state=42,n_estimators=500,learning_rate=1.0,max_depth=10,subsample=1.0,max_features=1.0)\n",
    "    models['rpe']=BaggingClassifier(bootstrap=False, max_features=10,n_estimators=500,max_samples=0.8,random_state=42)\n",
    "    members = [(n,m) for n,m in models.items()]\n",
    "    return members\n",
    "\n",
    "def fit_ensemble(models, X_train, X_val, y_train, y_val):\n",
    "\n",
    "    meta_X = list()\n",
    "    for _, model in models:\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        yhat = model.predict_proba(X_val)\n",
    "\n",
    "        meta_X.append(yhat)\n",
    "\n",
    "    meta_X = hstack(meta_X)\n",
    "\n",
    "    blender = GradientBoostingClassifier(random_state=42,n_estimators=500)\n",
    "\n",
    "    blender.fit(meta_X, y_val)\n",
    "    return blender\n",
    "\n",
    "def predict_ensemble(models, blender, X_test):\n",
    " # make predictions with base models\n",
    "    meta_X = list()\n",
    "    for _, model in models:\n",
    " # predict with base model\n",
    "        yhat = model.predict_proba(X_test)\n",
    " # store prediction\n",
    "        meta_X.append(yhat)\n",
    " # create 2d array from predictions, each set is an input feature\n",
    "    meta_X = hstack(meta_X)\n",
    " # predict\n",
    "    return blender.predict(meta_X)\n",
    "\n",
    "def predict_ensemble2(models, blender, X_test):\n",
    " # make predictions with base models\n",
    "    meta_X = list()\n",
    "    for _, model in models:\n",
    " # predict with base model\n",
    "        yhat = model.predict_proba(X_test)\n",
    " # store prediction\n",
    "        meta_X.append(yhat)\n",
    " # create 2d array from predictions, each set is an input feature\n",
    "    meta_X = hstack(meta_X)\n",
    " # predict\n",
    "    return meta_X\n",
    "\n",
    "\n",
    "X_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "print('Train: %s, Val: %s, Test: %s' % (X_train1.shape, X_val.shape, X_test.shape))\n",
    " # create the base models\n",
    "models = get_models()\n",
    " # train the blending ensemble\n",
    "blender = fit_ensemble(models, X_train1, X_val, y_train1, y_val)\n",
    "\n",
    "predictions = predict_ensemble(models, blender, X_train)\n",
    " # make predictions on test set\n",
    "f1 = f1_score(y_train, predictions, average='weighted')\n",
    "accuracy=accuracy_score(y_train, predictions)\n",
    "precision = precision_score(y_train, predictions,average=\"weighted\")\n",
    "recall = recall_score(y_train, predictions,average=\"weighted\")\n",
    "cm = confusion_matrix(y_train, predictions)\n",
    "predictions_prob = blender.predict_proba(predict_ensemble2(models,blender, X_train))[:, 1]\n",
    "\n",
    "fpr2, tpr2, _ = roc_curve(y_train,\n",
    "                          predictions_prob,\n",
    "                          pos_label = 1)\n",
    "auc1 = auc(fpr2, tpr2)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Accuracy\", accuracy)\n",
    "print(\"F1 Score\", f1)\n",
    "print(\"Recall\", recall)\n",
    "print(\"Precision\",precision)\n",
    "print(\"Area Under the Curve\",auc1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
